{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Ar9ejtjHeD"
      },
      "outputs": [],
      "source": [
        "! pip install Wikipedia-API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zQioUN8QNK9Y"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os \n",
        "import time \n",
        "\n",
        "\"\"\"DB & progress paths (better for the files to be stored in the drive\n",
        " in case you're using a notebook on the cloud for limited sessions)\"\"\"\n",
        "\n",
        "pathbd= \"/content/drive/MyDrive/DB\" # Change this\n",
        "pathp = \"/content/drive/MyDrive/ProgressScraping\"\n",
        "\n",
        "titles = pd.read_csv(r'/content/drive/MyDrive/ProgressScraping/titles.csv', sep='\\t')\n",
        "\n",
        "\n",
        "article_titles = titles.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Check if progress file exists\n",
        "progress_file = os.path.join(pathp,\"scraping_progress.json\")\n",
        "progress = {}\n",
        "try:\n",
        "    with open(progress_file, 'r') as file:\n",
        "        progress = json.load(file)\n",
        "except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "#depening on what you're looking for you may  want to not remove this\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove links\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    \n",
        "    # Remove special characters or unwanted symbols\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Trim leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Function to scrape the content of a Wikipedia article\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "        language='', # language\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "def scrape_wikipedia_article(article_title):\n",
        "\n",
        "    p_wiki = wiki_wiki.page(article_title)\n",
        "    if p_wiki.exists():\n",
        "      return p_wiki.text\n",
        "    else :\n",
        "      return False\n",
        "\n",
        "# Scrape the content for each article title and save as text files\n",
        "# Scrape the content for each article title\n",
        "pbar = tqdm(article_titles)\n",
        "for title in pbar:\n",
        "    if title in progress:\n",
        "        # Skip already scraped articles\n",
        "        pbar.set_postfix({'status': 'Skipped'})\n",
        "        continue\n",
        "    \n",
        "    # Scrape the article\n",
        "    time.sleep(0.5)\n",
        "    try :\n",
        "      article_content = scrape_wikipedia_article(title)\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "    if article_content:\n",
        "        # Save the content in a text file\n",
        "        filename = f\"{title.replace('/','_')}.txt\"\n",
        "        filename = os.path.join(pathbd,filename)\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(clean_text(article_content))\n",
        "        \n",
        "        # Update the progress\n",
        "        progress[title] = True\n",
        "        with open(progress_file, 'w') as file:\n",
        "            json.dump(progress, file)\n",
        "        \n",
        "        pbar.set_postfix({'status': 'Scraped'})\n",
        "        print(f\"Content saved for article: {title}\")\n",
        "    else:\n",
        "        pbar.set_postfix({'status': 'Failed'})\n",
        "        print(f\"Failed to scrape content for article: {title}\")\n",
        "\n",
        "pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yeSvi-9gXT6"
      },
      "outputs": [],
      "source": [
        "\"\"\"Some help with the post processing of the text\"\"\"\n",
        "# Set the directory path in Google Drive\n",
        "directory_path = '/content/drive/MyDrive/DB'\n",
        "\n",
        "# List all the files in the directory\n",
        "files = os.listdir(directory_path)\n",
        "\n",
        "# Specify the strings to search for in the filenames\n",
        "strings_to_search = ['something'] #some ids to removes like \"user:\" '\n",
        "\n",
        "# Iterate over the files and remove the ones matching the strings\n",
        "for file_name in files:\n",
        "    for string in strings_to_search:\n",
        "        if string in file_name:\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            os.remove(file_path)\n",
        "            print(f\"Removed file: {file_name}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(directory_path)\n",
        "print(len(files))"
      ],
      "metadata": {
        "id": "dcKM0-EGqv_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nLZhBTO3MENnMTzzZ_S5yYtYlJ0Ba018",
      "authorship_tag": "ABX9TyM0loX0Q8YvLmhPJd+S0SWS"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}